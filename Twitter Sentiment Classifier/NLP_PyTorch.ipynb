{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from time import sleep\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import torch\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from GPUtil import showUtilization as gpu_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = 'D:/MSC/NLP/NLP_ASS2/semeval-tweets/'\n",
    "data_train  = pd.read_pickle(BASE_DIR + 'train_data.pkl')\n",
    "data_test = pd.read_pickle(BASE_DIR + 'test_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDD_LEN = 100\n",
    "vocab = {'PAD':np.random.normal(scale=0.6, size=(EMBEDD_LEN, ))}\n",
    "vocab.update(pickle.load(open(BASE_DIR + 'LSTM_VOCAB_MAX.pkl', \"rb\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['clean_text'] = data_train['clean_text'].apply(lambda x: [key for key, grp in itertools.groupby([ i if i in vocab else 'UNK' for i in x ])] )\n",
    "data_test['clean_text'] = data_test['clean_text'].apply(lambda x: [key for key, grp in itertools.groupby([ i if i in vocab else 'UNK' for i in x ])] )\n",
    "\n",
    "data_train['nm_all_tok']= data_train['clean_text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['nm_all_tok'].quantile(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LENGTH = 1 #data_train['nm_all_tok'].quantile(0.01)\n",
    "MAX_LENGTH = 33 #data_train['nm_all_tok'].quantile(0.99)\n",
    "data_train = data_train[~(data_train['nm_all_tok'] < MIN_LENGTH) | (data_train['nm_all_tok'] > MAX_LENGTH)]\n",
    "# data_test = data_test[~(data_test['nm_all_tok'] < MIN_LENGTH) | (data_test['nm_all_tok'] > MAX_LENGTH)]\n",
    "\n",
    "\n",
    "vocab['UNK'] = np.random.normal(scale=0.6, size=(EMBEDD_LEN, ))\n",
    "word_to_idx = dict(zip(vocab.keys(),range(len(vocab.keys()))))\n",
    "weights_matrix = torch.from_numpy(np.vstack(list(vocab.values())))\n",
    "\n",
    "data_train['clean_text'] = data_train['clean_text'].apply(lambda x: [ word_to_idx[item] for item in x ])\n",
    "data_test['clean_text'] = data_test['clean_text'].apply(lambda x: [ word_to_idx[item] for item in x ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "binariser = LabelBinarizer()\n",
    "train_y = binariser.fit_transform(data_train['sentiment'].values)\n",
    "test_y = binariser.transform(data_test['sentiment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47026\n",
      "3531\n"
     ]
    }
   ],
   "source": [
    "DATA_TRAIN = list(zip(list(data_train['clean_text'].values),train_y))\n",
    "DATA_TEST = list(zip(list(data_test['clean_text'].values),test_y))\n",
    "print(len(DATA_TRAIN))\n",
    "print(len(DATA_TEST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    x, y = zip(*batch)  \n",
    "    x = list(map(lambda x: torch.tensor(x), x))\n",
    "    lens = list(map(len, x))\n",
    "    padded = pad_sequence(x, batch_first=True)\n",
    "    y = torch.from_numpy(np.vstack(list(map(lambda x: torch.tensor(x), y))))\n",
    "    return padded, y,lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2000\n",
    "loader_train = DataLoader(DATA_TRAIN, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn,drop_last=True)\n",
    "test_x,test_y,test_lengths = custom_collate_fn(DATA_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  1% |  2% |\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")\n",
    "\n",
    "gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = torch.nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "class ToyNN1(torch.nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, num_layers,output_size):\n",
    "        super().__init__()\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True,dropout=0.3,bidirectional=True)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.hidden_layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(256, output_size),)\n",
    "        self.hidden_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(256, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(64, output_size))\n",
    "        \n",
    "        \n",
    "    def forward(self, inp,lens):\n",
    "        if next(self.parameters()).is_cuda:\n",
    "            pad_embed = self.embedding(inp).to(device)\n",
    "        else:\n",
    "            pad_embed = self.embedding(inp)\n",
    "        pad_embed_pack = pack_padded_sequence(pad_embed, lens, batch_first=True, enforce_sorted=False)\n",
    "        seq, (ht, ct) = self.lstm(pad_embed_pack)\n",
    "        out = self.dropout(torch.cat((ht[-1],ht[-2]), 1))\n",
    "        out = self.hidden_layer1(out)\n",
    "        return torch.nn.functional.softmax(out,dim=-1)\n",
    "    \n",
    "    \n",
    "class ToyNN2(torch.nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, num_layers,output_size,train_on_gpu=False):\n",
    "        super().__init__()\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.n_layers = num_layers\n",
    "        self.gpu = train_on_gpu\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, self.hidden_dim, self.n_layers, batch_first=True,dropout=0.2)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.fc = torch.nn.Linear(self.hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, inp,lens,hidden_input):\n",
    "        batch_size = inp.size(0)\n",
    "        pad_embed = self.embedding(inp).to(device)\n",
    "        lstm_out, hidden = self.lstm(pad_embed, hidden_input)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        sig_out = torch.nn.functional.softmax(out,dim=-1)\n",
    "        sig_out = sig_out.view(batch_size,max(lens), -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        return sig_out,hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        if (self.gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToyNN1(\n",
      "  (embedding): Embedding(5000, 100)\n",
      "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (hidden_layer1): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=3, bias=True)\n",
      "  )\n",
      "  (hidden_layer): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  3% | 16% |\n"
     ]
    }
   ],
   "source": [
    "model = ToyNN1(weights_matrix,128,2,3)\n",
    "model.to(device)\n",
    "print(model)\n",
    "gpu_usage()\n",
    "\n",
    "lr=0.001\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "clip = 5\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Loss: 1.1442606372392696, Test Loss: 1.0599970817565918, Train Acc: 0.23590635828458584, Test Acc: 0.34114682452261746\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 75% | 22% |\n",
      "==================================================\n",
      "Epoch 1 Train Loss: 1.0620851716774962, Test Loss: 1.0165778398513794, Train Acc: 0.41465414630361036, Test Acc: 0.5208610638289094\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 56% | 22% |\n",
      "==================================================\n",
      "Epoch 2 Train Loss: 1.0256423516066178, Test Loss: 0.9771782755851746, Train Acc: 0.5046273736200669, Test Acc: 0.5352990565225474\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 70% | 22% |\n",
      "==================================================\n",
      "Epoch 3 Train Loss: 1.0105097393328728, Test Loss: 0.9558832049369812, Train Acc: 0.5208094577845105, Test Acc: 0.5583432201964141\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 76% | 22% |\n",
      "==================================================\n",
      "Epoch 4 Train Loss: 0.9914127637147904, Test Loss: 0.9257593750953674, Train Acc: 0.5422387045708854, Test Acc: 0.5944438314384254\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 54% | 22% |\n",
      "==================================================\n",
      "Epoch 5 Train Loss: 0.9853935504970344, Test Loss: 0.9313362836837769, Train Acc: 0.5442174561592071, Test Acc: 0.5767439457254411\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 53% | 22% |\n",
      "==================================================\n",
      "Epoch 6 Train Loss: 0.9867194345075151, Test Loss: 0.9031622409820557, Train Acc: 0.5431278304980663, Test Acc: 0.6195697652845847\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 70% | 22% |\n",
      "==================================================\n",
      "Epoch 7 Train Loss: 0.9724475626751132, Test Loss: 0.9352618455886841, Train Acc: 0.5586129341082076, Test Acc: 0.58620783264986\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 71% | 22% |\n",
      "==================================================\n",
      "Epoch 8 Train Loss: 0.9717026050466558, Test Loss: 0.9294166564941406, Train Acc: 0.5603405696877165, Test Acc: 0.5955746111317571\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 76% | 22% |\n",
      "==================================================\n",
      "Epoch 9 Train Loss: 0.9684292203654413, Test Loss: 0.9020223021507263, Train Acc: 0.5662144204807653, Test Acc: 0.6198146574566334\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 77% | 22% |\n",
      "==================================================\n",
      "Epoch 10 Train Loss: 0.9612339061058086, Test Loss: 0.894923985004425, Train Acc: 0.5728590931656881, Test Acc: 0.6202467162957004\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 78% | 22% |\n",
      "==================================================\n",
      "Epoch 11 Train Loss: 0.9614797912330731, Test Loss: 0.9120281338691711, Train Acc: 0.5703221536858394, Test Acc: 0.6069266581882858\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 55% | 22% |\n",
      "==================================================\n",
      "Epoch 12 Train Loss: 0.9506758531280186, Test Loss: 0.9369652271270752, Train Acc: 0.5819906548437007, Test Acc: 0.5880132787808088\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 75% | 22% |\n",
      "==================================================\n",
      "Epoch 13 Train Loss: 0.9521464847598387, Test Loss: 0.9118553996086121, Train Acc: 0.5825282578451719, Test Acc: 0.6087546221238517\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 74% | 22% |\n",
      "==================================================\n",
      "Epoch 14 Train Loss: 0.9465754007658232, Test Loss: 0.9152401685714722, Train Acc: 0.5894744455491233, Test Acc: 0.6094489288392989\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 78% | 22% |\n",
      "==================================================\n",
      "Epoch 15 Train Loss: 0.9439071151951085, Test Loss: 0.8903140425682068, Train Acc: 0.5915046582812337, Test Acc: 0.6291396678888798\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 70% | 22% |\n",
      "==================================================\n",
      "Epoch 16 Train Loss: 0.94190483197829, Test Loss: 0.8997770547866821, Train Acc: 0.5928730331719397, Test Acc: 0.6257189131467213\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 69% | 22% |\n",
      "==================================================\n",
      "Epoch 17 Train Loss: 0.9440055727038694, Test Loss: 0.9006516337394714, Train Acc: 0.5925535767567841, Test Acc: 0.6143990896158446\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 74% | 22% |\n",
      "==================================================\n",
      "Epoch 18 Train Loss: 0.9394363278513369, Test Loss: 0.8936665058135986, Train Acc: 0.5958789706319888, Test Acc: 0.631361956155574\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 71% | 22% |\n",
      "==================================================\n",
      "Epoch 19 Train Loss: 0.9321402876714001, Test Loss: 0.9076529741287231, Train Acc: 0.6041847067371345, Test Acc: 0.617369691039254\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 72% | 22% |\n",
      "==================================================\n",
      "Epoch 20 Train Loss: 0.9262067558415558, Test Loss: 0.9053861498832703, Train Acc: 0.6121740263927774, Test Acc: 0.621208277250895\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 68% | 22% |\n",
      "==================================================\n",
      "Epoch 21 Train Loss: 0.9198277113256247, Test Loss: 0.9208863973617554, Train Acc: 0.618236226272913, Test Acc: 0.608409235298286\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 74% | 22% |\n",
      "==================================================\n",
      "Epoch 22 Train Loss: 0.9181029178357643, Test Loss: 0.8951462507247925, Train Acc: 0.6209242088228578, Test Acc: 0.6260686000259574\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 72% | 22% |\n",
      "==================================================\n",
      "Epoch 23 Train Loss: 0.9172127047064512, Test Loss: 0.9197789430618286, Train Acc: 0.6218889192155775, Test Acc: 0.6038208512107394\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 68% | 22% |\n",
      "==================================================\n",
      "Epoch 24 Train Loss: 0.9189557628255823, Test Loss: 0.8936575055122375, Train Acc: 0.6202117147651937, Test Acc: 0.6341683597308213\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 71% | 22% |\n",
      "==================================================\n",
      "Epoch 25 Train Loss: 0.9127431914637918, Test Loss: 0.8939031362533569, Train Acc: 0.6261847086000482, Test Acc: 0.6261807450287913\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 76% | 22% |\n",
      "==================================================\n",
      "Epoch 26 Train Loss: 0.9080547289511431, Test Loss: 0.9001425504684448, Train Acc: 0.6320913635961816, Test Acc: 0.6208562446709935\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 69% | 22% |\n",
      "==================================================\n",
      "Epoch 27 Train Loss: 0.9069375114039234, Test Loss: 0.8950226902961731, Train Acc: 0.6322352830690046, Test Acc: 0.6300899268286381\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 73% | 22% |\n",
      "==================================================\n",
      "Epoch 28 Train Loss: 0.9001050457034422, Test Loss: 0.9017003774642944, Train Acc: 0.6395352926413443, Test Acc: 0.623899273302733\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 70% | 22% |\n",
      "==================================================\n",
      "Epoch 29 Train Loss: 0.898159208321053, Test Loss: 0.8909381031990051, Train Acc: 0.6424889812126534, Test Acc: 0.6327473528100099\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 71% | 22% |\n",
      "==================================================\n",
      "Epoch 30 Train Loss: 0.8939155146699884, Test Loss: 0.8997657895088196, Train Acc: 0.6470181098431127, Test Acc: 0.6239384002955651\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 77% | 22% |\n",
      "==================================================\n",
      "Epoch 31 Train Loss: 0.8999580157787903, Test Loss: 0.9036607146263123, Train Acc: 0.6413462820872083, Test Acc: 0.6193788238811789\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 64% | 22% |\n",
      "==================================================\n",
      "Epoch 32 Train Loss: 0.8920557498620904, Test Loss: 0.8862656354904175, Train Acc: 0.6491977280905318, Test Acc: 0.6320873600243506\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 71% | 22% |\n",
      "==================================================\n",
      "Epoch 33 Train Loss: 0.8792696208707664, Test Loss: 0.9067327380180359, Train Acc: 0.6635434293269246, Test Acc: 0.6175768315741755\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 76% | 22% |\n",
      "==================================================\n",
      "Epoch 34 Train Loss: 0.8897491123456022, Test Loss: 0.9103373885154724, Train Acc: 0.6517146038340429, Test Acc: 0.6110575041454568\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 75% | 22% |\n",
      "==================================================\n",
      "Epoch 35 Train Loss: 0.8916524655365425, Test Loss: 0.8815562129020691, Train Acc: 0.6483917210133291, Test Acc: 0.6412519987593392\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 72% | 22% |\n",
      "==================================================\n",
      "Epoch 36 Train Loss: 0.8748203520269497, Test Loss: 0.8990417122840881, Train Acc: 0.6687649518836748, Test Acc: 0.6243068035796819\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 74% | 22% |\n",
      "==================================================\n",
      "Epoch 37 Train Loss: 0.8698831770536692, Test Loss: 0.8991709351539612, Train Acc: 0.6734075043927462, Test Acc: 0.6221759621834518\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 72% | 22% |\n",
      "==================================================\n",
      "Epoch 38 Train Loss: 0.8655503989017528, Test Loss: 0.8907896876335144, Train Acc: 0.6782990102377392, Test Acc: 0.6347764980351833\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 73% | 22% |\n",
      "==================================================\n",
      "Epoch 39 Train Loss: 0.8696975153736446, Test Loss: 0.8857854604721069, Train Acc: 0.6728755085072448, Test Acc: 0.6360757991007268\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 75% | 22% |\n",
      "==================================================\n",
      "Epoch 40 Train Loss: 0.8639357779544333, Test Loss: 0.8931290507316589, Train Acc: 0.6798571328930603, Test Acc: 0.6285861007082447\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 72% | 22% |\n",
      "==================================================\n",
      "Epoch 41 Train Loss: 0.8614990193740182, Test Loss: 0.8847869634628296, Train Acc: 0.6825225608462008, Test Acc: 0.6378334925680832\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 73% | 22% |\n",
      "==================================================\n",
      "Epoch 42 Train Loss: 0.8569122677173303, Test Loss: 0.887222409248352, Train Acc: 0.6867406882352975, Test Acc: 0.6406065687554483\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 71% | 22% |\n",
      "==================================================\n",
      "Epoch 43 Train Loss: 0.861247053956208, Test Loss: 0.8839393258094788, Train Acc: 0.6830157472503595, Test Acc: 0.6366006046462365\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 74% | 22% |\n",
      "==================================================\n",
      "Epoch 44 Train Loss: 0.8592628595569859, Test Loss: 0.9045266509056091, Train Acc: 0.6841139381986237, Test Acc: 0.6242250828851801\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 67% | 22% |\n",
      "==================================================\n",
      "Epoch 45 Train Loss: 0.8507182915145937, Test Loss: 0.9089503288269043, Train Acc: 0.6939052525069673, Test Acc: 0.6171550374505533\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 78% | 22% |\n",
      "==================================================\n",
      "Epoch 46 Train Loss: 0.8440463543443576, Test Loss: 0.9009941816329956, Train Acc: 0.7018172889786207, Test Acc: 0.6236072380840387\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 71% | 22% |\n",
      "==================================================\n",
      "Epoch 47 Train Loss: 0.8356350542695626, Test Loss: 0.9022129774093628, Train Acc: 0.7098207401601718, Test Acc: 0.6273111508232998\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 74% | 22% |\n",
      "==================================================\n",
      "Epoch 48 Train Loss: 0.837712545927452, Test Loss: 0.8949728012084961, Train Acc: 0.707757620293243, Test Acc: 0.6294437818017403\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 73% | 22% |\n",
      "==================================================\n",
      "Epoch 49 Train Loss: 0.835680119075205, Test Loss: 0.895049512386322, Train Acc: 0.7093508754739618, Test Acc: 0.6278118043300229\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 76% | 22% |\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "train_acces,train_losses,test_acces,test_losses = [],[],[],[]\n",
    "for epoch in range(epochs):\n",
    "    train_outputs,train_labels = [],[]\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "    for inputs,labels,lengths in loader_train:\n",
    "        sample_weight = torch.from_numpy(compute_sample_weight(class_weight='balanced',y=labels)).to(device)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)   \n",
    "        model.zero_grad()\n",
    "        output = model(inputs,lengths)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss = (loss * sample_weight).mean()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_outputs.append(output.cpu().detach().numpy())\n",
    "        train_labels.append(labels.cpu().detach().numpy())\n",
    "        del inputs, labels, output, loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        train_outputs = np.vstack(train_outputs)\n",
    "        train_y = np.vstack(train_labels)\n",
    "        train_loss = criterion(torch.from_numpy(train_outputs.squeeze()),torch.from_numpy(train_y.astype(float)) ).mean()\n",
    "        train_losses.append(train_loss)\n",
    "        train_acc = f1_score( np.argmax(train_y,axis=-1) , np.argmax(train_outputs,axis=-1) ,average='macro')\n",
    "        train_acces.append(train_acc)\n",
    "\n",
    "        test_outputs = model(test_x.to(device),test_lengths).cpu()\n",
    "        test_loss = criterion(test_outputs.squeeze(), test_y.float()).mean()\n",
    "        test_losses.append(test_loss)\n",
    "        test_acc = f1_score( np.argmax(test_y,axis=-1) , torch.argmax(test_outputs,dim=-1) ,average='macro')\n",
    "        test_acces.append(test_acc)\n",
    "\n",
    "\n",
    "        print('''Epoch {epoch} Train Loss: {train_loss}, Test Loss: {test_loss}, Train Acc: {train_acc}, Test Acc: {test_acc}'''\\\n",
    "              .format(epoch=epoch,train_loss=train_loss,test_loss=test_loss,train_acc=train_acc,test_acc=test_acc))\n",
    "        del test_outputs, test_loss,train_outputs, train_labels,train_loss\n",
    "        torch.cuda.empty_cache()\n",
    "        gpu_usage()\n",
    "        print(25*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,l = next(iter(loader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(x.to(device),l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion1(out.squeeze(), y.to(device).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7369, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7252, device='cuda:0', dtype=torch.float64, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(loss * weight).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion1 = torch.nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.from_numpy(compute_sample_weight(class_weight='balanced',y=y)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x255950d7860>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24df9484ac8>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToyNN2(\n",
      "  (embedding): Embedding(5000, 100)\n",
      "  (lstm): LSTM(100, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=100, out_features=3, bias=True)\n",
      ")\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  3% | 16% |\n"
     ]
    }
   ],
   "source": [
    "model = ToyNN2(weights_matrix,100,2,3,True)\n",
    "model.to(device)\n",
    "print(model)\n",
    "gpu_usage()\n",
    "\n",
    "lr=0.001\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "clip = 5\n",
    "epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Loss: 1.0529532072621006, Test Loss: 1.0432411432266235, Train Acc: 0.21919591580366568, Test Acc: 0.22379144709371254\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 73% | 19% |\n",
      "==================================================\n",
      "Epoch 1 Train Loss: 1.045135930698405, Test Loss: 1.0488736629486084, Train Acc: 0.21740352867200272, Test Acc: 0.19963212313530623\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 61% | 20% |\n",
      "==================================================\n",
      "Epoch 2 Train Loss: 1.0442332244957646, Test Loss: 1.0434054136276245, Train Acc: 0.21107936707093974, Test Acc: 0.19913935782853356\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 50% | 20% |\n",
      "==================================================\n",
      "Epoch 3 Train Loss: 1.0439888608545385, Test Loss: 1.0425844192504883, Train Acc: 0.2101967646134921, Test Acc: 0.19963212313530623\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 44% | 20% |\n",
      "==================================================\n",
      "Epoch 4 Train Loss: 1.0435631383247272, Test Loss: 1.0384944677352905, Train Acc: 0.21107031208756089, Test Acc: 0.19957818726837537\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 74% | 20% |\n",
      "==================================================\n",
      "Epoch 5 Train Loss: 1.0392379034270522, Test Loss: 1.0253877639770508, Train Acc: 0.2639256641715411, Test Acc: 0.34089216802631345\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 39% | 20% |\n",
      "==================================================\n",
      "Epoch 6 Train Loss: 1.0304649478568826, Test Loss: 1.0160866975784302, Train Acc: 0.339219324651836, Test Acc: 0.3660986523775513\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 83% | 19% |\n",
      "==================================================\n",
      "Epoch 7 Train Loss: 1.02273846889824, Test Loss: 1.0244501829147339, Train Acc: 0.3559601108712201, Test Acc: 0.35525460154675687\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 66% | 19% |\n",
      "==================================================\n",
      "Epoch 8 Train Loss: 1.014972671096043, Test Loss: 1.0120975971221924, Train Acc: 0.3650787926888455, Test Acc: 0.36775137033018385\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 48% | 19% |\n",
      "==================================================\n",
      "Epoch 9 Train Loss: 1.0160367906734507, Test Loss: 1.0141067504882812, Train Acc: 0.36568374559800043, Test Acc: 0.37093446342194253\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 77% | 21% |\n",
      "==================================================\n",
      "Epoch 10 Train Loss: 1.0105327922054517, Test Loss: 1.042763590812683, Train Acc: 0.36993030661902954, Test Acc: 0.2665910636667096\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 45% | 20% |\n",
      "==================================================\n",
      "Epoch 11 Train Loss: 1.0061700818243846, Test Loss: 1.009461760520935, Train Acc: 0.3717779591850385, Test Acc: 0.36872541603630865\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 51% | 20% |\n",
      "==================================================\n",
      "Epoch 12 Train Loss: 1.002058786248648, Test Loss: 1.007495403289795, Train Acc: 0.37644586290516613, Test Acc: 0.3650162135992465\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 53% | 19% |\n",
      "==================================================\n",
      "Epoch 13 Train Loss: 0.9972773069361205, Test Loss: 1.0056244134902954, Train Acc: 0.3793653773099934, Test Acc: 0.3638846925469054\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 74% | 19% |\n",
      "==================================================\n",
      "Epoch 14 Train Loss: 0.9967204913772563, Test Loss: 1.0144438743591309, Train Acc: 0.38137735943476114, Test Acc: 0.38129024703408954\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 55% | 19% |\n",
      "==================================================\n",
      "Epoch 15 Train Loss: 1.0016563297253782, Test Loss: 1.0015310049057007, Train Acc: 0.37728997544123993, Test Acc: 0.36958892111416836\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 73% | 19% |\n",
      "==================================================\n",
      "Epoch 16 Train Loss: 0.989558077140521, Test Loss: 1.0011264085769653, Train Acc: 0.411814181001042, Test Acc: 0.43908269740009026\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 65% | 19% |\n",
      "==================================================\n",
      "Epoch 17 Train Loss: 0.9807982475975509, Test Loss: 0.9924631118774414, Train Acc: 0.4777650366596003, Test Acc: 0.4673474759414867\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 72% | 19% |\n",
      "==================================================\n",
      "Epoch 18 Train Loss: 0.9737342476703787, Test Loss: 0.988695502281189, Train Acc: 0.5017600806649437, Test Acc: 0.43787798231691966\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 65% | 19% |\n",
      "==================================================\n",
      "Epoch 19 Train Loss: 0.9693056311209997, Test Loss: 0.9858594536781311, Train Acc: 0.5090513774933001, Test Acc: 0.4639079006009217\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 38% | 19% |\n",
      "==================================================\n",
      "Epoch 20 Train Loss: 0.9717704897901064, Test Loss: 1.02739679813385, Train Acc: 0.5050281345615345, Test Acc: 0.3811141920347889\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 65% | 19% |\n",
      "==================================================\n",
      "Epoch 21 Train Loss: 0.9776978973842436, Test Loss: 0.986470639705658, Train Acc: 0.49631467232623544, Test Acc: 0.4799809517687155\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 52% | 19% |\n",
      "==================================================\n",
      "Epoch 22 Train Loss: 0.9674771938887976, Test Loss: 0.9837402701377869, Train Acc: 0.5128670061445777, Test Acc: 0.46666442422795407\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 53% | 20% |\n",
      "==================================================\n",
      "Epoch 23 Train Loss: 0.9664290294454944, Test Loss: 0.9787299633026123, Train Acc: 0.5190955349496985, Test Acc: 0.4756067821182885\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 51% | 19% |\n",
      "==================================================\n",
      "Epoch 24 Train Loss: 0.9601219750937595, Test Loss: 0.9805179834365845, Train Acc: 0.5303382767323196, Test Acc: 0.48438861520143583\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 55% | 19% |\n",
      "==================================================\n",
      "Epoch 25 Train Loss: 0.9576817947536387, Test Loss: 0.9771900177001953, Train Acc: 0.5264067106554006, Test Acc: 0.5076851921130751\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 40% | 20% |\n",
      "==================================================\n",
      "Epoch 26 Train Loss: 0.9552927245401567, Test Loss: 0.9777681231498718, Train Acc: 0.5370663648424674, Test Acc: 0.48884898963487583\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 48% | 19% |\n",
      "==================================================\n",
      "Epoch 27 Train Loss: 0.953101504016948, Test Loss: 0.9770984649658203, Train Acc: 0.53585369502509, Test Acc: 0.4878686266754087\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 38% | 20% |\n",
      "==================================================\n",
      "Epoch 28 Train Loss: 0.9508508805908182, Test Loss: 0.9779187440872192, Train Acc: 0.5391258200425949, Test Acc: 0.478523178514497\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 46% | 19% |\n",
      "==================================================\n",
      "Epoch 29 Train Loss: 0.948973709329482, Test Loss: 0.976107656955719, Train Acc: 0.5414231084587778, Test Acc: 0.4857588027778266\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 57% | 19% |\n",
      "==================================================\n",
      "Epoch 30 Train Loss: 0.9477608991092251, Test Loss: 0.9766103625297546, Train Acc: 0.5486572873995855, Test Acc: 0.4870638858502441\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 57% | 19% |\n",
      "==================================================\n",
      "Epoch 31 Train Loss: 0.947287324581095, Test Loss: 0.9758792519569397, Train Acc: 0.5442011860410796, Test Acc: 0.4939873842826201\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 56% | 20% |\n",
      "==================================================\n",
      "Epoch 32 Train Loss: 0.9440054964250134, Test Loss: 0.9735455513000488, Train Acc: 0.5552231041514785, Test Acc: 0.49572404710426493\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 48% | 20% |\n",
      "==================================================\n",
      "Epoch 33 Train Loss: 0.9403021116346442, Test Loss: 0.9766144752502441, Train Acc: 0.5584357816142557, Test Acc: 0.4762022711126203\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 66% | 19% |\n",
      "==================================================\n",
      "Epoch 34 Train Loss: 0.9419899187241831, Test Loss: 0.980840265750885, Train Acc: 0.5541358154805982, Test Acc: 0.4681771140100686\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 47% | 19% |\n",
      "==================================================\n",
      "Epoch 35 Train Loss: 0.9374350667961182, Test Loss: 0.9771960973739624, Train Acc: 0.5615117306291189, Test Acc: 0.5054409211996141\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 61% | 19% |\n",
      "==================================================\n",
      "Epoch 36 Train Loss: 0.9360445984986521, Test Loss: 0.9810706377029419, Train Acc: 0.5675756635520297, Test Acc: 0.4788417942952307\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 72% | 19% |\n",
      "==================================================\n",
      "Epoch 37 Train Loss: 0.9365892602564186, Test Loss: 0.9790960550308228, Train Acc: 0.5622712570496832, Test Acc: 0.48483191090644734\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 60% | 19% |\n",
      "==================================================\n",
      "Epoch 38 Train Loss: 0.9314601796621917, Test Loss: 0.9784119725227356, Train Acc: 0.570805347124529, Test Acc: 0.5001318142254915\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 53% | 20% |\n",
      "==================================================\n",
      "Epoch 39 Train Loss: 0.9298782621199085, Test Loss: 0.9806765913963318, Train Acc: 0.5711167360098078, Test Acc: 0.4801772749917997\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 49% | 20% |\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "train_acces,train_losses,test_acces,test_losses = [],[],[],[]\n",
    "for epoch in range(epochs):\n",
    "    train_outputs,train_labels = [],[]\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "    h = model.init_hidden(BATCH_SIZE)\n",
    "    for inputs,labels,lengths in loader_train:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        h = tuple([each.data for each in h])\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs,lengths,h)\n",
    "        torch.cuda.empty_cache()\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_outputs.append(output.cpu().detach().numpy())\n",
    "        train_labels.append(labels.cpu().detach().numpy())\n",
    "        del inputs, labels, output, loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        train_outputs = np.vstack(train_outputs)\n",
    "        train_y = np.vstack(train_labels)\n",
    "        train_loss = criterion(torch.from_numpy(train_outputs.squeeze()),torch.from_numpy(train_y.astype(float)) )\n",
    "        train_losses.append(train_loss)\n",
    "        train_acc = f1_score( np.argmax(train_y,axis=-1) , np.argmax(train_outputs,axis=-1) ,average='macro')\n",
    "        train_acces.append(train_acc)\n",
    "\n",
    "        val_h = model.init_hidden(len(DATA_TEST))\n",
    "        c = tuple([each.data for each in val_h])\n",
    "        test_outputs,_ = model(test_x.to(device),test_lengths,val_h)\n",
    "        test_outputs = test_outputs.cpu()\n",
    "        test_loss = criterion(test_outputs.squeeze(), test_y.float())\n",
    "        test_losses.append(test_loss)\n",
    "        test_acc = f1_score( np.argmax(test_y,axis=-1) , torch.argmax(test_outputs,dim=-1) ,average='macro')\n",
    "        test_acces.append(test_acc)\n",
    "\n",
    "\n",
    "        print('''Epoch {epoch} Train Loss: {train_loss}, Test Loss: {test_loss}, Train Acc: {train_acc}, Test Acc: {test_acc}'''\\\n",
    "              .format(epoch=epoch,train_loss=train_loss,test_loss=test_loss,train_acc=train_acc,test_acc=test_acc))\n",
    "        del test_outputs, test_loss,train_outputs, train_labels,train_loss\n",
    "        torch.cuda.empty_cache()\n",
    "        gpu_usage()\n",
    "        print(25*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3531"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_dict = {}\n",
    "# with open(BASE_DIR + \"glove.6B.100d.txt\", 'r', encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         values = line.split()\n",
    "#         token = values[0]\n",
    "#         vector = np.asarray(values[1:], \"float32\")\n",
    "#         embeddings_dict[token] = vector\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# count_vector = CountVectorizer(tokenizer=lambda x: x,preprocessor=lambda x: x)\n",
    "# count_vector.fit(data_train['clean_text'])\n",
    "\n",
    "# vocab = count_vector.vocabulary_\n",
    "\n",
    "# min_list,max_list = [],[]\n",
    "# while len(vocab) > 0:\n",
    "#     min_word = min(vocab,key=vocab.get)\n",
    "#     min_list.append(min_word)\n",
    "#     vocab.pop(min_word)\n",
    "#     if len(vocab) == 0:\n",
    "#         break\n",
    "#     max_word = max(vocab,key=vocab.get)\n",
    "#     max_list.append(max_word)\n",
    "#     vocab.pop(max_word)\n",
    "    \n",
    "# new_vocab = {}\n",
    "# while len(new_vocab) != 4998:\n",
    "#     word = max_list[-1]\n",
    "#     vector = embeddings_dict.get(word)\n",
    "#     if vector is None :\n",
    "#         max_list.pop()\n",
    "#     else:\n",
    "#         new_vocab[word] = vector\n",
    "#         max_list.pop()\n",
    "#         if len(new_vocab) == 4998:\n",
    "#             break\n",
    "#     word = min_list[-1]\n",
    "#     vector = embeddings_dict.get(word)\n",
    "#     if vector is None:\n",
    "#         min_list.pop()\n",
    "#     else:\n",
    "#         new_vocab[word] = vector\n",
    "#         min_list.pop()\n",
    "#         if len(new_vocab) == 4998:\n",
    "#             break\n",
    "            \n",
    "# pickle.dump(new_vocab,open(BASE_DIR + 'LSTM_VOCAB.pkl', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Contractions\n",
    "# file = open(BASE_DIR + 'CONTRACTIONS.pkl', \"rb\")\n",
    "# CONTRACTION_MAP = pickle.load(file)\n",
    "# sleep(1)\n",
    "# del file\n",
    "# contraction = sorted(CONTRACTION_MAP, key=len, reverse=True)\n",
    "# c_re = re.compile('(%s)' % '|'.join(contraction))\n",
    "# def expandContractions(text, c_re=c_re):\n",
    "#     def replace(match):\n",
    "#         return CONTRACTION_MAP[match.group(0)]\n",
    "#     return c_re.sub(replace, text.lower())\n",
    "\n",
    "# ## Emojis Regex\n",
    "# file = open(BASE_DIR + 'EMOJIS.pkl', \"rb\")\n",
    "# EMOJIS = pickle.load(file)\n",
    "# sleep(1)\n",
    "# del file\n",
    "# emojis = sorted(EMOJIS, key=len, reverse=True)\n",
    "# pattern = '|'.join(re.escape(u) for u in emojis)\n",
    "# regex = re.compile(pattern, re.U)\n",
    "\n",
    "# ## Regex expression for Booster Increase and Decrease words\n",
    "# B_INC = ['exceptionally', 'substantially', 'considerable', 'considerably', 'particularly', 'tremendously', 'unbelievably',\\\n",
    "#          'exceptional', 'absolutely', 'completely', 'enormously', 'especially', 'fabulously', 'incredible', 'incredibly',\\\n",
    "#          'remarkably', 'thoroughly', 'tremendous', 'amazingly', 'decidedly', 'extremely', 'intensely', 'unusually',\\\n",
    "#          'enormous', 'entirely', 'flipping', 'fracking', 'fricking', 'frigging', 'awfully', 'extreme', 'flippin',\\\n",
    "#          'frackin', 'frickin', 'friggin', 'fucking', 'fugging', 'greatly', 'majorly', 'totally', 'utterly', 'deeply',\\\n",
    "#          'effing', 'fuckin', 'fuggin', 'highly', 'hugely', 'purely', 'really', 'fully', 'hella', 'major', 'quite',\\\n",
    "#          'total', 'utter', 'more', 'most', 'uber', 'very', 'so']\n",
    "# B_DEC = ['almost', 'barely', 'hardly', 'just enough', 'kind of', 'kinda', 'kindof', 'kind-of', 'less', 'little',\\\n",
    "#          'marginal', 'marginally', 'occasional', 'occasionally', 'partly', 'scarce', 'scarcely', 'slight', 'slightly',\\\n",
    "#          'somewhat', 'sort of', 'sorta', 'sortof', 'sort-of']\n",
    "# pattern_binc = '|'.join(re.escape(u) for u in B_INC)\n",
    "# pattern_bdec = '|'.join(re.escape(u) for u in B_DEC)\n",
    "\n",
    "# ## Reading EXICON DICT \n",
    "# file = open(BASE_DIR + 'VADER_LEXICONS_SCORE.pkl', \"rb\")\n",
    "# LEXICON_DICT = pickle.load(file)\n",
    "# sleep(1)\n",
    "# del file\n",
    "\n",
    "\n",
    "# file = open(BASE_DIR + 'POSITIVE.pkl', \"rb\")\n",
    "# POS_LIST = pickle.load(file)\n",
    "# sleep(1)\n",
    "# del file\n",
    "# POS_LIST = sorted(POS_LIST, key=len, reverse=True)\n",
    "# pattern_pos = '|'.join(re.escape(u) for u in POS_LIST)\n",
    "\n",
    "# file = open(BASE_DIR + 'NEGATIVE.pkl', \"rb\")\n",
    "# NEG_LIST = pickle.load(file)\n",
    "# sleep(1)\n",
    "# del file\n",
    "# NEG_LIST = sorted(NEG_LIST, key=len, reverse=True)\n",
    "# pattern_neg = '|'.join(re.escape(u) for u in NEG_LIST)\n",
    "\n",
    "# file = open(BASE_DIR + 'BAD.pkl', \"rb\")\n",
    "# BAD_LIST = pickle.load(file)\n",
    "# sleep(1)\n",
    "# del file\n",
    "# BAD_LIST = sorted(BAD_LIST, key=len, reverse=True)\n",
    "# pattern_bad = '|'.join(re.escape(u) for u in BAD_LIST)\n",
    "\n",
    "# def return_tok_val(sentence):\n",
    "#     pos_tok = []\n",
    "#     neg_tok = []\n",
    "#     tokens = twokenize.tokenizeRawTweetText(sentence)\n",
    "#     ln_all_tokens = len(tokens)\n",
    "#     for toke in tokens:\n",
    "#         val = LEXICON_DICT.get(toke)\n",
    "#         if val:\n",
    "#             if val >0:\n",
    "#                 pos_tok.append(val)\n",
    "#             else:\n",
    "#                 neg_tok.append(val)\n",
    "    \n",
    "#     return tokens,ln_all_tokens,len(pos_tok),len(neg_tok),sum(pos_tok),sum(neg_tok)\n",
    "\n",
    "# def clean_data(data,col,re_inc_boostr,re_dec_boostr,re_pos,re_neg,re_bad):\n",
    "#     dataframe = data.copy()\n",
    "#     link_regex = re.compile(r'(?:ftp|https?|www|file)\\.?:?[//|\\\\\\\\]?[\\w\\d:#@%/;$()~_?\\+-=\\\\\\&]+\\.[\\w\\d:#@%/;$~_?\\+-=\\\\\\&]+')\n",
    "#     dataframe = dataframe.assign(**dict(zip([col,'num_link'], zip(*dataframe['text'].apply(lambda x: re.subn(link_regex,'LINK',x) ) ))))\n",
    "#     dataframe = dataframe.assign(**dict(zip([col,'num_usermention'], zip(*dataframe[col].apply(lambda x: re.subn(r'@[\\w]*','USERMENTION',x)) ))))\n",
    "#     dataframe = dataframe.assign(**dict(zip([col,'num_hashtag'], zip(*dataframe[col].apply(lambda x: re.subn(r'#[\\w]*','HASHTAG',x) ) ))))\n",
    "#     dataframe = dataframe.assign(**dict(zip([col,'num_emoji'], zip(*dataframe[col].apply(lambda x: re.subn(regex, lambda m: EMOJIS.get(m.group(), 'EMOJI') , x) )))))\n",
    "#     dataframe[col] = dataframe[col].apply(lambda x : re.sub(r'(.)\\1{2,}', r'\\1',x)) # make looong as long\n",
    "#     dataframe[col] = dataframe[col].apply(lambda x : expandContractions(x) ) # expand contracts\n",
    "#     dataframe[col] = dataframe[col].str.lower()\n",
    "#     dataframe['NUM_INC_BOOSTR'] = dataframe[col].apply(lambda x : len(re.findall(re_inc_boostr,x)) ) # Booster Increasing Words\n",
    "#     dataframe['NUM_DEC_BOOSTR'] = dataframe[col].apply(lambda x : len(re.findall(re_dec_boostr,x)) ) # Booster Increasing Words\n",
    "#     dataframe['NUM_POS_WORDS'] = dataframe[col].apply(lambda x : len(re.findall(re_pos,x)) ) # Number of Positive words\n",
    "#     dataframe['NUM_NEG_WORDS'] = dataframe[col].apply(lambda x : len(re.findall(re_neg,x)) ) # Number of Positive words\n",
    "#     dataframe['NUM_BAD_WORDS'] = dataframe[col].apply(lambda x : len(re.findall(re_bad,x)) ) # Number of Positive words\n",
    "#     dataframe = dataframe.assign(**dict(zip([col,'num_exclaim'], zip(*dataframe[col].apply(lambda x: re.subn(r\"[!]\",'',x)  ) ))))  \n",
    "#     dataframe = dataframe.assign(**dict(zip([col,'num_punct'], zip(*dataframe[col].apply(lambda x: re.subn(r\"['\\\".?!,:;]\",'',x)  ) ))))  \n",
    "\n",
    "#     dataframe = dataframe.assign(**dict(zip([col,'nm_all_tok','nm_pos_tok','nm_neg_tok','sm_pos_tok','sum_nrg_tok'], zip(*dataframe[col].apply(lambda x: return_tok_val(x) ) ))))\n",
    "#     dataframe['lex_tok'] = dataframe['nm_pos_tok'] + dataframe['nm_neg_tok']\n",
    "#     dataframe['total_lex_score'] = dataframe['sm_pos_tok'] + dataframe['sum_nrg_tok']\n",
    "#     dataframe['r_sco/tok'] = dataframe['total_lex_score']/dataframe['lex_tok']\n",
    "#     for col in dataframe.columns.difference(['clean_text','text','sentiment']):\n",
    "#         dataframe[col] += abs(dataframe[col].min())\n",
    "    \n",
    "#     dataframe = dataframe.fillna(0)\n",
    "    \n",
    "#     return dataframe\n",
    "\n",
    "# df_train = pd.read_csv(BASE_DIR + 'twitter-training-data.txt',sep='\\t',names=['ID','sentiment','text']).drop('ID',axis=1)\n",
    "# df_dev = pd.read_csv(BASE_DIR + 'twitter-dev-data.txt',sep='\\t',names=['ID','sentiment','text']).drop('ID',axis=1)\n",
    "# df_train = pd.concat([df_train,df_dev]).reset_index(drop=True)\n",
    "# del df_dev\n",
    "# df_test = pd.read_csv(BASE_DIR + 'twitter-test1.txt',sep='\\t',names=['ID','sentiment','text']).drop('ID',axis=1)\n",
    "\n",
    "\n",
    "# df_train['clean_text'] = df_train['text']\n",
    "# df_test['clean_text'] = df_test['text']\n",
    "\n",
    "# df_train = clean_data(df_train,'clean_text',pattern_binc,pattern_bdec,pattern_pos,pattern_neg,pattern_bad)\n",
    "# df_test = clean_data(df_test,'clean_text',pattern_binc,pattern_bdec,pattern_pos,pattern_neg,pattern_bad)\n",
    "\n",
    "# replace = {'positive': 0, 'negative': 1, 'neutral': 2}\n",
    "# df_train['sentiment'] = df_train['sentiment'].replace(replace)\n",
    "# df_test['sentiment'] = df_test['sentiment'].replace(replace)\n",
    "\n",
    "# df_train.to_pickle(BASE_DIR + 'train_data.pkl')\n",
    "# df_test.to_pickle(BASE_DIR + 'test_data.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
