{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a97b2c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "from time import sleep\n",
    "import os\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bda8be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('D:/MSC/NLP/NLP_ASS2/semeval-tweets/')\n",
    "import twokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85934659",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = 'D:/MSC/NLP/NLP_ASS2/semeval-tweets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b43078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Contractions\n",
    "file = open(BASE_DIR + 'CONTRACTIONS.pkl', \"rb\")\n",
    "CONTRACTION_MAP = pickle.load(file)\n",
    "sleep(1)\n",
    "del file\n",
    "contraction = sorted(CONTRACTION_MAP, key=len, reverse=True)\n",
    "# c_re = re.compile('(%s)' % '|'.join(contraction))\n",
    "c_re = re.compile('|'.join(r'\\b'+re.escape(u)+r'\\b' for u in contraction))\n",
    "\n",
    "\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return CONTRACTION_MAP[match.group(0)]\n",
    "    return c_re.sub(replace, text.lower())\n",
    "\n",
    "## Emojis Regex\n",
    "file = open(BASE_DIR + 'EMOJIS.pkl', \"rb\")\n",
    "EMOJIS = pickle.load(file)\n",
    "sleep(1)\n",
    "del file\n",
    "emojis = sorted(EMOJIS, key=len, reverse=True)\n",
    "pattern = '|'.join(re.escape(u) for u in emojis)\n",
    "regex_emoji = re.compile(pattern, re.U)\n",
    "\n",
    "## Regex expression for Booster Increase and Decrease words\n",
    "B_INC = ['exceptionally', 'substantially', 'considerable', 'considerably', 'particularly', 'tremendously', 'unbelievably',\\\n",
    "         'exceptional', 'absolutely', 'completely', 'enormously', 'especially', 'fabulously', 'incredible', 'incredibly',\\\n",
    "         'remarkably', 'thoroughly', 'tremendous', 'amazingly', 'decidedly', 'extremely', 'intensely', 'unusually',\\\n",
    "         'enormous', 'entirely', 'flipping', 'fracking', 'fricking', 'frigging', 'awfully', 'extreme', 'flippin',\\\n",
    "         'frackin', 'frickin', 'friggin', 'fucking', 'fugging', 'greatly', 'majorly', 'totally', 'utterly', 'deeply',\\\n",
    "         'effing', 'fuckin', 'fuggin', 'highly', 'hugely', 'purely', 'really', 'fully', 'hella', 'major', 'quite',\\\n",
    "         'total', 'utter', 'more', 'most', 'uber', 'very', 'so']\n",
    "B_DEC = ['almost', 'barely', 'hardly', 'just enough', 'kind of', 'kinda', 'kindof', 'kind-of', 'less', 'little',\\\n",
    "         'marginal', 'marginally', 'occasional', 'occasionally', 'partly', 'scarce', 'scarcely', 'slight', 'slightly',\\\n",
    "         'somewhat', 'sort of', 'sorta', 'sortof', 'sort-of']\n",
    "# pattern_binc = '|'.join(re.escape(u) for u in B_INC)\n",
    "pattern_binc = re.compile('|'.join(r'\\b'+re.escape(u)+r'\\b' for u in B_INC))\n",
    "# pattern_bdec = '|'.join(re.escape(u) for u in B_DEC)\n",
    "pattern_bdec = re.compile('|'.join(r'\\b'+re.escape(u)+r'\\b' for u in B_DEC))\n",
    "\n",
    "## Reading EXICON DICT \n",
    "file = open(BASE_DIR + 'VADER_LEXICONS_SCORE.pkl', \"rb\")\n",
    "LEXICON_DICT = pickle.load(file)\n",
    "sleep(1)\n",
    "del file\n",
    "\n",
    "\n",
    "file = open(BASE_DIR + 'POSITIVE.pkl', \"rb\")\n",
    "POS_LIST = pickle.load(file)\n",
    "sleep(1)\n",
    "del file\n",
    "POS_LIST = sorted(POS_LIST, key=len, reverse=True)\n",
    "# pattern_pos = '|'.join(re.escape(u) for u in POS_LIST)\n",
    "pattern_pos = re.compile('|'.join(r'\\b'+re.escape(u)+r'\\b' for u in POS_LIST))\n",
    "\n",
    "\n",
    "file = open(BASE_DIR + 'NEGATIVE.pkl', \"rb\")\n",
    "NEG_LIST = pickle.load(file)\n",
    "sleep(1)\n",
    "del file\n",
    "NEG_LIST = sorted(NEG_LIST, key=len, reverse=True)\n",
    "# pattern_neg = '|'.join(re.escape(u) for u in NEG_LIST)\n",
    "pattern_neg = re.compile('|'.join(r'\\b'+re.escape(u)+r'\\b' for u in NEG_LIST))\n",
    "\n",
    "\n",
    "file = open(BASE_DIR + 'BAD.pkl', \"rb\")\n",
    "BAD_LIST = pickle.load(file)\n",
    "sleep(1)\n",
    "del file\n",
    "BAD_LIST = sorted(BAD_LIST, key=len, reverse=True)\n",
    "# pattern_bad = '|'.join(re.escape(u) for u in BAD_LIST)\n",
    "pattern_bad = re.compile('|'.join(r'\\b'+re.escape(u)+r'\\b' for u in BAD_LIST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f571aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_tok_val(sentence):\n",
    "    pos_tok = []\n",
    "    neg_tok = []\n",
    "    tokens = twokenize.tokenizeRawTweetText(sentence)\n",
    "    ln_all_tokens = len(tokens)\n",
    "    for toke in tokens:\n",
    "        val = LEXICON_DICT.get(toke)\n",
    "        if val:\n",
    "            if val >0:\n",
    "                pos_tok.append(val)\n",
    "            else:\n",
    "                neg_tok.append(val)\n",
    "    \n",
    "    return tokens,ln_all_tokens,len(pos_tok),len(neg_tok),sum(pos_tok),sum(neg_tok)\n",
    "\n",
    "def clean_data(data,col,re_emoji,re_inc_boostr,re_dec_boostr,re_pos,re_neg,re_bad):\n",
    "    dataframe = data.copy()\n",
    "    link_regex = re.compile(r'(?:ftp|https?|www|file)\\.?:?[//|\\\\\\\\]?[\\w\\d:#@%/;$()~_?\\+-=\\\\\\&]+\\.[\\w\\d:#@%/;$~_?\\+-=\\\\\\&]+')\n",
    "    dataframe = dataframe.assign(**dict(zip([col,'num_link'], zip(*dataframe['text'].apply(lambda x: re.subn(link_regex,'LINK',x) ) ))))\n",
    "    dataframe = dataframe.assign(**dict(zip([col,'num_usermention'], zip(*dataframe[col].apply(lambda x: re.subn(r'@[\\w]*','USERMENTION',x)) ))))\n",
    "    dataframe = dataframe.assign(**dict(zip([col,'num_hashtag'], zip(*dataframe[col].apply(lambda x: re.subn(r'#[\\w]*','HASHTAG',x) ) ))))\n",
    "    dataframe = dataframe.assign(**dict(zip([col,'num_emoji'], zip(*dataframe[col].apply(lambda x: re.subn(re_emoji, lambda m: EMOJIS.get(m.group(), 'EMOJI') , x) )))))\n",
    "    dataframe[col] = dataframe[col].apply(lambda x : re.sub(r'(.)\\1{2,}', r'\\1',x)) # make looong as long\n",
    "    dataframe[col] = dataframe[col].apply(lambda x : expandContractions(x) ) # expand contracts\n",
    "    dataframe[col] = dataframe[col].str.lower()\n",
    "    dataframe['NUM_INC_BOOSTR'] = dataframe[col].apply(lambda x : len(re.findall(re_inc_boostr,x)) ) # Booster Increasing Words\n",
    "    dataframe['NUM_DEC_BOOSTR'] = dataframe[col].apply(lambda x : len(re.findall(re_dec_boostr,x)) ) # Booster Increasing Words\n",
    "    dataframe['NUM_POS_WORDS'] = dataframe[col].apply(lambda x : len(re.findall(re_pos,x)) ) # Number of Positive words\n",
    "    dataframe['NUM_NEG_WORDS'] = dataframe[col].apply(lambda x : len(re.findall(re_neg,x)) ) # Number of Positive words\n",
    "    dataframe['NUM_BAD_WORDS'] = dataframe[col].apply(lambda x : len(re.findall(re_bad,x)) ) # Number of Positive words\n",
    "    dataframe = dataframe.assign(**dict(zip([col,'num_exclaim'], zip(*dataframe[col].apply(lambda x: re.subn(r\"[!]\",'',x)  ) ))))  \n",
    "    dataframe = dataframe.assign(**dict(zip([col,'num_punct'], zip(*dataframe[col].apply(lambda x: re.subn(r\"['\\\"“”‘’.?!…,:;]\",'',x)  ) ))))  \n",
    "    \n",
    "    dataframe = dataframe.assign(**dict(zip([col,'nm_all_tok','nm_pos_tok','nm_neg_tok','sm_pos_tok','sum_nrg_tok'], zip(*dataframe[col].apply(lambda x: return_tok_val(x) ) ))))\n",
    "    dataframe['lex_tok'] = dataframe['nm_pos_tok'] + dataframe['nm_neg_tok']\n",
    "    dataframe['total_lex_score'] = dataframe['sm_pos_tok'] + dataframe['sum_nrg_tok']\n",
    "    dataframe['r_sco/tok'] = dataframe['total_lex_score']/dataframe['lex_tok']\n",
    "    dataframe['sum_nrg_tok'] = dataframe['sum_nrg_tok'].abs()\n",
    "    \n",
    "    for col in dataframe.columns.difference(['clean_text','text','sentiment','nm_all_tok']):\n",
    "        dataframe[col] = dataframe[col]/dataframe['nm_all_tok']\n",
    "    for col in dataframe.columns.difference(['clean_text','text','sentiment']):\n",
    "        dataframe[col] += abs(dataframe[col].min())\n",
    "        dataframe[col] = dataframe[col]/dataframe[col].max()   \n",
    "    dataframe = dataframe.fillna(0)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ffb467",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(BASE_DIR + 'twitter-training-data.txt',sep='\\t',names=['ID','sentiment','text']).drop('ID',axis=1)\n",
    "df_dev = pd.read_csv(BASE_DIR + 'twitter-dev-data.txt',sep='\\t',names=['ID','sentiment','text']).drop('ID',axis=1)\n",
    "df_test1 = pd.read_csv(BASE_DIR + 'twitter-test1.txt',sep='\\t',names=['ID','sentiment','text']).drop('ID',axis=1)\n",
    "df_test2 = pd.read_csv(BASE_DIR + 'twitter-test2.txt',sep='\\t',names=['ID','sentiment','text']).drop('ID',axis=1)\n",
    "df_test3 = pd.read_csv(BASE_DIR + 'twitter-test3.txt',sep='\\t',names=['ID','sentiment','text']).drop('ID',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab215d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data processed\n",
      "Developemnt data processed\n",
      "Test1 data processed\n",
      "Test2 data processed\n",
      "Test3 data processed\n"
     ]
    }
   ],
   "source": [
    "df_train['clean_text'] = df_train['text']\n",
    "df_train = clean_data(df_train,'clean_text',regex_emoji,pattern_binc,pattern_bdec,pattern_pos,pattern_neg,pattern_bad)\n",
    "print('Train data processed')\n",
    "\n",
    "df_dev['clean_text'] = df_dev['text']\n",
    "df_dev = clean_data(df_dev,'clean_text',regex_emoji,pattern_binc,pattern_bdec,pattern_pos,pattern_neg,pattern_bad)\n",
    "print('Developemnt data processed')\n",
    "\n",
    "df_test1['clean_text'] = df_test1['text']\n",
    "df_test1 = clean_data(df_test1,'clean_text',regex_emoji,pattern_binc,pattern_bdec,pattern_pos,pattern_neg,pattern_bad)\n",
    "print('Test1 data processed')\n",
    "\n",
    "df_test2['clean_text'] = df_test2['text']\n",
    "df_test2 = clean_data(df_test2,'clean_text',regex_emoji,pattern_binc,pattern_bdec,pattern_pos,pattern_neg,pattern_bad)\n",
    "print('Test2 data processed')\n",
    "\n",
    "df_test3['clean_text'] = df_test3['text']\n",
    "df_test3 = clean_data(df_test3,'clean_text',regex_emoji,pattern_binc,pattern_bdec,pattern_pos,pattern_neg,pattern_bad)\n",
    "print('Test3 data processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94f027dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace = dict(zip(df_train['sentiment'].unique(),range(len(df_train['sentiment'].unique()))))\n",
    "df_train['sentiment'] = df_train['sentiment'].replace(replace)\n",
    "df_dev['sentiment'] = df_dev['sentiment'].replace(replace)\n",
    "df_test1['sentiment'] = df_test1['sentiment'].replace(replace)\n",
    "df_test2['sentiment'] = df_test2['sentiment'].replace(replace)\n",
    "df_test3['sentiment'] = df_test3['sentiment'].replace(replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8070480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=lambda x: x,preprocessor=lambda x: x,ngram_range=(1,3),min_df=10)\n",
    "tfidf_vectorizer_vectors_train = tfidf.fit_transform(df_train.loc[:,'clean_text'].values)\n",
    "tfidf_vectorizer_vectors_dev = tfidf.transform(df_dev.loc[:,'clean_text'].values)\n",
    "tfidf_vectorizer_vectors_test1 = tfidf.transform(df_test1.loc[:,'clean_text'].values)\n",
    "tfidf_vectorizer_vectors_test2 = tfidf.transform(df_test2.loc[:,'clean_text'].values)\n",
    "tfidf_vectorizer_vectors_test3 = tfidf.transform(df_test3.loc[:,'clean_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "938eea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = tfidf_vectorizer_vectors_train.toarray()\n",
    "dev_x = tfidf_vectorizer_vectors_dev.toarray()\n",
    "test1_x = tfidf_vectorizer_vectors_test1.toarray()\n",
    "test2_x = tfidf_vectorizer_vectors_test2.toarray()\n",
    "test3_x = tfidf_vectorizer_vectors_test3.toarray()\n",
    "\n",
    "\n",
    "train_y = df_train['sentiment'].values\n",
    "dev_y = df_dev['sentiment'].values\n",
    "test1_y = df_test1['sentiment'].values\n",
    "test2_y = df_test2['sentiment'].values\n",
    "test3_y = df_test3['sentiment'].values\n",
    "\n",
    "train_x = np.concatenate((train_x,df_train[df_train.columns.difference(['sentiment', 'text', 'clean_text'])].values),axis=1)\n",
    "dev_x = np.concatenate((dev_x,df_dev[df_dev.columns.difference(['sentiment', 'text', 'clean_text'])].values),axis=1)\n",
    "test1_x = np.concatenate((test1_x,df_test1[df_test1.columns.difference(['sentiment', 'text', 'clean_text'])].values),axis=1)\n",
    "test2_x = np.concatenate((test2_x,df_test2[df_test2.columns.difference(['sentiment', 'text', 'clean_text'])].values),axis=1)\n",
    "test3_x = np.concatenate((test3_x,df_test3[df_test3.columns.difference(['sentiment', 'text', 'clean_text'])].values),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec5ae168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5b9046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on Train data 0.7797991365405622\n",
      "F1 Score on Train data 0.6376087403662589\n",
      "F1 Score on Test1 data 0.701197191127753\n",
      "F1 Score on Test2 data 0.6975931538023925\n",
      "F1 Score on Test3 data 0.6690442962168911\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=300).fit(train_x,train_y) #,sample\n",
    "print('F1 Score on Train data',f1_score(train_y,model.predict(train_x),average='weighted'))\n",
    "print('F1 Score on Dev data',f1_score(dev_y,model.predict(dev_x),average='weighted'))\n",
    "print('F1 Score on Test1 data',f1_score(test1_y,model.predict(test1_x),average='weighted'))\n",
    "print('F1 Score on Test2 data',f1_score(test2_y,model.predict(test2_x),average='weighted'))\n",
    "print('F1 Score on Test3 data',f1_score(test3_y,model.predict(test3_x),average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "62b7e4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on Train data 0.7049961251375948\n",
      "F1 Score on Dev data 0.646387800145492\n",
      "F1 Score on Test1 data 0.6334959724093925\n",
      "F1 Score on Test2 data 0.6851088436160981\n",
      "F1 Score on Test3 data 0.6225931218142564\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB().fit(train_x,train_y)\n",
    "print('F1 Score on Train data',f1_score(train_y,model.predict(train_x),average='weighted'))\n",
    "print('F1 Score on Dev data',f1_score(dev_y,model.predict(dev_x),average='weighted'))\n",
    "print('F1 Score on Test1 data',f1_score(test1_y,model.predict(test1_x),average='weighted'))\n",
    "print('F1 Score on Test2 data',f1_score(test2_y,model.predict(test2_x),average='weighted'))\n",
    "print('F1 Score on Test3 data',f1_score(test3_y,model.predict(test3_x),average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "880e402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6aeb3e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on Train data 0.857551049026708\n",
      "F1 Score on Dev data 0.6541147310239087\n",
      "F1 Score on Test1 data 0.6816422461948877\n",
      "F1 Score on Test2 data 0.7009861923840657\n",
      "F1 Score on Test3 data 0.6433326337341967\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(max_iter=1000).fit(train_x,train_y)\n",
    "print('F1 Score on Train data',f1_score(train_y,model.predict(train_x),average='weighted'))\n",
    "print('F1 Score on Dev data',f1_score(dev_y,model.predict(dev_x),average='weighted'))\n",
    "print('F1 Score on Test1 data',f1_score(test1_y,model.predict(test1_x),average='weighted'))\n",
    "print('F1 Score on Test2 data',f1_score(test2_y,model.predict(test2_x),average='weighted'))\n",
    "print('F1 Score on Test3 data',f1_score(test3_y,model.predict(test3_x),average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a1c0b29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tfidf_vectorizer_vectors_train.toarray()\n",
    "vocab = dict(zip(tfidf.get_feature_names(),features[:,:len(tfidf.get_feature_names())].sum(axis=0)))\n",
    "\n",
    "embeddings_dict = {}\n",
    "with open(\"glove.6B.100d.txt\", 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        token = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[token] = vector\n",
    "\n",
    "\n",
    "new_vocab = {}\n",
    "while len(new_vocab) != 4998:\n",
    "    word = max(vocab,key=vocab.get)\n",
    "    vector = embeddings_dict.get(word)\n",
    "    if vector is None :\n",
    "        vocab.pop(word)\n",
    "    else:\n",
    "        new_vocab[word] = vector\n",
    "        vocab.pop(word)\n",
    "        if len(new_vocab) == 4998:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "61852849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shorter Vocab is Read\n"
     ]
    }
   ],
   "source": [
    "EMBEDD_LEN = 100\n",
    "vocab = {'PAD':np.random.normal(scale=0.6, size=(EMBEDD_LEN, ))}\n",
    "vocab.update(new_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6f16ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7fe79b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['clean_text'] = df_train['clean_text'].apply(lambda x: [key for key, grp in itertools.groupby([ i if i in vocab else 'UNK' for i in x ])] )\n",
    "df_train['nm_all_tok']= df_train['clean_text'].apply(lambda x: len(x))\n",
    "\n",
    "df_dev['clean_text'] = df_dev['clean_text'].apply(lambda x: [key for key, grp in itertools.groupby([ i if i in vocab else 'UNK' for i in x ])] )\n",
    "df_test1['clean_text'] = df_test1['clean_text'].apply(lambda x: [key for key, grp in itertools.groupby([ i if i in vocab else 'UNK' for i in x ])] )\n",
    "df_test2['clean_text'] = df_test2['clean_text'].apply(lambda x: [key for key, grp in itertools.groupby([ i if i in vocab else 'UNK' for i in x ])] )\n",
    "df_test3['clean_text'] = df_test3['clean_text'].apply(lambda x: [key for key, grp in itertools.groupby([ i if i in vocab else 'UNK' for i in x ])] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "251e7225",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LENGTH = 1 #data_train['nm_all_tok'].quantile(0.01)\n",
    "MAX_LENGTH = 33 #data_train['nm_all_tok'].quantile(0.99)\n",
    "df_train = df_train[~(df_train['nm_all_tok'] < MIN_LENGTH) | (df_train['nm_all_tok'] > MAX_LENGTH)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0df06924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d2a83e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab['UNK'] = np.random.normal(scale=0.6, size=(EMBEDD_LEN, ))\n",
    "word_to_idx = dict(zip(vocab.keys(),range(len(vocab.keys()))))\n",
    "weights_matrix = torch.from_numpy(np.vstack(list(vocab.values())))\n",
    "\n",
    "df_train['clean_text'] = df_train['clean_text'].apply(lambda x: [ word_to_idx[item] for item in x ])\n",
    "df_dev['clean_text'] = df_dev['clean_text'].apply(lambda x: [ word_to_idx[item] for item in x ])\n",
    "df_test1['clean_text'] = df_test1['clean_text'].apply(lambda x: [ word_to_idx[item] for item in x ])\n",
    "df_test2['clean_text'] = df_test2['clean_text'].apply(lambda x: [ word_to_idx[item] for item in x ])\n",
    "df_test3['clean_text'] = df_test3['clean_text'].apply(lambda x: [ word_to_idx[item] for item in x ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "974c53b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fb26913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "binariser = LabelBinarizer()\n",
    "train_y = binariser.fit_transform(df_train['sentiment'].values)\n",
    "dev_y = binariser.transform(df_dev['sentiment'].values)\n",
    "test_y1 = binariser.transform(df_test1['sentiment'].values)\n",
    "test_y2 = binariser.transform(df_test2['sentiment'].values)\n",
    "test_y3 = binariser.transform(df_test3['sentiment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0fa8540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch,y_none=False):\n",
    "    if y_none:\n",
    "        x = batch\n",
    "    else:\n",
    "        x, y = zip(*batch)\n",
    "        y = torch.from_numpy(np.vstack(list(map(lambda x: torch.tensor(x), y))))\n",
    "    x = list(map(lambda x: torch.tensor(x), x))\n",
    "    lens = list(map(len, x))\n",
    "    padded = pad_sequence(x, batch_first=True)\n",
    "    if y_none:\n",
    "        return padded,lens\n",
    "    else:\n",
    "        return padded, y,lens\n",
    "\n",
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = torch.nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "class ToyNN1(torch.nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, num_layers,output_size):\n",
    "        super().__init__()\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True,dropout=0.3,bidirectional=True)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.hidden_layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(256, output_size),)\n",
    "        self.hidden_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(256, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(64, output_size))\n",
    "        \n",
    "        \n",
    "    def forward(self, inp,lens):\n",
    "        if next(self.parameters()).is_cuda:\n",
    "            pad_embed = self.embedding(inp).to(device)\n",
    "        else:\n",
    "            pad_embed = self.embedding(inp)\n",
    "        pad_embed_pack = pack_padded_sequence(pad_embed, lens, batch_first=True, enforce_sorted=False)\n",
    "        seq, (ht, ct) = self.lstm(pad_embed_pack)\n",
    "        out = self.dropout(torch.cat((ht[-1],ht[-2]), 1))\n",
    "        out = self.hidden_layer1(out)\n",
    "        return torch.nn.functional.softmax(out,dim=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "79431560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45026\n",
      "2000\n",
      "3531\n",
      "1853\n",
      "2379\n"
     ]
    }
   ],
   "source": [
    "DATA_TRAIN = list(zip(list(df_train['clean_text'].values),train_y))\n",
    "DATA_DEV = list(zip(list(df_dev['clean_text'].values),dev_y))\n",
    "DATA_TEST1 = list(zip(list(df_test1['clean_text'].values),test_y1))\n",
    "DATA_TEST2 = list(zip(list(df_test2['clean_text'].values),test_y2))\n",
    "DATA_TEST3 = list(zip(list(df_test3['clean_text'].values),test_y3))\n",
    "print(len(DATA_TRAIN))\n",
    "print(len(DATA_DEV))\n",
    "print(len(DATA_TEST1))\n",
    "print(len(DATA_TEST2))\n",
    "print(len(DATA_TEST3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "860d3dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7fd17f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500\n",
    "loader_train = DataLoader(DATA_TRAIN, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn,drop_last=True)\n",
    "dev_x,dev_y,dev_length = custom_collate_fn(DATA_DEV)\n",
    "test_x1,test_y1,test_lengths1 = custom_collate_fn(DATA_TEST1)\n",
    "test_x2,test_y2,test_lengths2 = custom_collate_fn(DATA_TEST2)\n",
    "test_x3,test_y3,test_lengths3 = custom_collate_fn(DATA_TEST3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "63905ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "682cf290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToyNN1(\n",
      "  (embedding): Embedding(5000, 100)\n",
      "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (hidden_layer1): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=3, bias=True)\n",
      "  )\n",
      "  (hidden_layer): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ToyNN1(weights_matrix,128,2,3)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "lr=0.001\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "clip = 5\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c647f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f4ef243b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE MAXIMUM TEST ACC IS 0.4886213783083792\n",
      "Epoch 0 Train Acc: 0.38676231116285437, Test1 Acc: 0.5010962542074545 Test2 Acc 0.5167452896003581 Test 3 Acc 0.4480225911173248 AVG Test ACC 0.4886213783083792\n",
      "==================================================\n",
      "AVERAGE MAXIMUM TEST ACC IS 0.5561683807991162\n",
      "Epoch 1 Train Acc: 0.5098300260248229, Test1 Acc: 0.5765023759682059 Test2 Acc 0.5555542273775739 Test 3 Acc 0.5364485390515689 AVG Test ACC 0.5561683807991162\n",
      "==================================================\n",
      "AVERAGE MAXIMUM TEST ACC IS 0.5597255855492057\n",
      "Epoch 2 Train Acc: 0.5369832538983298, Test1 Acc: 0.588279328481722 Test2 Acc 0.5689936499442806 Test 3 Acc 0.5219037782216144 AVG Test ACC 0.5597255855492057\n",
      "==================================================\n",
      "Epoch 3 Train Acc: 0.5499728557272713, Test1 Acc: 0.5867642798897008 Test2 Acc 0.5544837509550912 Test 3 Acc 0.52867246150474 AVG Test ACC 0.5566401641165107\n",
      "==================================================\n",
      "AVERAGE MAXIMUM TEST ACC IS 0.588495724258117\n",
      "Epoch 4 Train Acc: 0.5619357129738459, Test1 Acc: 0.6134902017584949 Test2 Acc 0.5907768104941149 Test 3 Acc 0.5612201605217412 AVG Test ACC 0.588495724258117\n",
      "==================================================\n",
      "Epoch 5 Train Acc: 0.5687127242784048, Test1 Acc: 0.6062682071010497 Test2 Acc 0.5895607376451073 Test 3 Acc 0.5359956791141456 AVG Test ACC 0.5772748746201009\n",
      "==================================================\n",
      "AVERAGE MAXIMUM TEST ACC IS 0.6106708966349009\n",
      "Epoch 6 Train Acc: 0.5791456837737433, Test1 Acc: 0.6393563751424726 Test2 Acc 0.6096575412492005 Test 3 Acc 0.5829987735130301 AVG Test ACC 0.6106708966349009\n",
      "==================================================\n",
      "Epoch 7 Train Acc: 0.5904487345987456, Test1 Acc: 0.6365669430790003 Test2 Acc 0.58979621634367 Test 3 Acc 0.5813136068238248 AVG Test ACC 0.602558922082165\n",
      "==================================================\n",
      "AVERAGE MAXIMUM TEST ACC IS 0.6166259296860938\n",
      "Epoch 8 Train Acc: 0.5940804253550833, Test1 Acc: 0.6467692817646444 Test2 Acc 0.6141200003710013 Test 3 Acc 0.5889885069226355 AVG Test ACC 0.6166259296860938\n",
      "==================================================\n",
      "Epoch 9 Train Acc: 0.6033173749428465, Test1 Acc: 0.6192054517877829 Test2 Acc 0.5910841807562816 Test 3 Acc 0.5835215963057067 AVG Test ACC 0.597937076283257\n",
      "==================================================\n",
      "Epoch 10 Train Acc: 0.6038654077496911, Test1 Acc: 0.6071277261752711 Test2 Acc 0.5842614784703923 Test 3 Acc 0.5642412875920796 AVG Test ACC 0.5852101640792476\n",
      "==================================================\n",
      "Epoch 11 Train Acc: 0.614867988143705, Test1 Acc: 0.6275668138206832 Test2 Acc 0.6028406443346852 Test 3 Acc 0.5661204189964578 AVG Test ACC 0.5988426257172753\n",
      "==================================================\n",
      "Epoch 12 Train Acc: 0.6175396105897569, Test1 Acc: 0.6410432865647332 Test2 Acc 0.6144297797921049 Test 3 Acc 0.5851615945455367 AVG Test ACC 0.6135448869674582\n",
      "==================================================\n",
      "Epoch 13 Train Acc: 0.6219930361655441, Test1 Acc: 0.6234007300107334 Test2 Acc 0.5944477128897931 Test 3 Acc 0.5650455965334212 AVG Test ACC 0.5942980131446493\n",
      "==================================================\n",
      "Epoch 14 Train Acc: 0.6314389071944623, Test1 Acc: 0.6384401647403256 Test2 Acc 0.6126224259108722 Test 3 Acc 0.578617294013081 AVG Test ACC 0.609893294888093\n",
      "==================================================\n",
      "Epoch 15 Train Acc: 0.6396052124883737, Test1 Acc: 0.6298313235776732 Test2 Acc 0.60327722812278 Test 3 Acc 0.5742211898309459 AVG Test ACC 0.602443247177133\n",
      "==================================================\n",
      "AVERAGE MAXIMUM TEST ACC IS 0.6247719760783261\n",
      "Epoch 16 Train Acc: 0.6498496664015628, Test1 Acc: 0.6509270083080584 Test2 Acc 0.6316708087891673 Test 3 Acc 0.5917181111377526 AVG Test ACC 0.6247719760783261\n",
      "==================================================\n",
      "Epoch 17 Train Acc: 0.652326125185411, Test1 Acc: 0.6373790706711849 Test2 Acc 0.6113832109812573 Test 3 Acc 0.5862671827987057 AVG Test ACC 0.6116764881503826\n",
      "==================================================\n",
      "Epoch 18 Train Acc: 0.6572593869964064, Test1 Acc: 0.6374149292808241 Test2 Acc 0.6104254258152001 Test 3 Acc 0.5652241850830072 AVG Test ACC 0.6043548467263439\n",
      "==================================================\n",
      "Epoch 19 Train Acc: 0.6604462896303113, Test1 Acc: 0.6334641522339596 Test2 Acc 0.6124554274481322 Test 3 Acc 0.5901687146833616 AVG Test ACC 0.6120294314551512\n",
      "==================================================\n",
      "Epoch 20 Train Acc: 0.6732134344515964, Test1 Acc: 0.6395365444089962 Test2 Acc 0.6131953232354206 Test 3 Acc 0.5768850432412344 AVG Test ACC 0.6098723036285504\n",
      "==================================================\n",
      "Epoch 21 Train Acc: 0.6736831880416597, Test1 Acc: 0.6428420692263282 Test2 Acc 0.6267500084390368 Test 3 Acc 0.5869230840571648 AVG Test ACC 0.6188383872408433\n",
      "==================================================\n",
      "Epoch 22 Train Acc: 0.6828692624161622, Test1 Acc: 0.6422679664433021 Test2 Acc 0.625973802866409 Test 3 Acc 0.5945592681587301 AVG Test ACC 0.6209336791561472\n",
      "==================================================\n",
      "Epoch 23 Train Acc: 0.6827408833534628, Test1 Acc: 0.6478313866787458 Test2 Acc 0.6166136017957303 Test 3 Acc 0.5933168355271629 AVG Test ACC 0.6192539413338797\n",
      "==================================================\n",
      "Epoch 24 Train Acc: 0.6970655393798495, Test1 Acc: 0.6339241062153315 Test2 Acc 0.6123695404278978 Test 3 Acc 0.5788210444589498 AVG Test ACC 0.6083715637007264\n",
      "==================================================\n",
      "Epoch 25 Train Acc: 0.7031248585242719, Test1 Acc: 0.6446627039108134 Test2 Acc 0.6237100687572387 Test 3 Acc 0.588493382193158 AVG Test ACC 0.6189553849537367\n",
      "==================================================\n",
      "Epoch 26 Train Acc: 0.7033221573648801, Test1 Acc: 0.6382495977082466 Test2 Acc 0.6256530055701508 Test 3 Acc 0.5877850363653984 AVG Test ACC 0.6172292132145986\n",
      "==================================================\n",
      "AVERAGE MAXIMUM TEST ACC IS 0.6253255666677761\n",
      "Epoch 27 Train Acc: 0.7128103693603632, Test1 Acc: 0.6569402722082739 Test2 Acc 0.6212309579039755 Test 3 Acc 0.597805469891079 AVG Test ACC 0.6253255666677761\n",
      "==================================================\n",
      "Epoch 28 Train Acc: 0.7185971354935488, Test1 Acc: 0.6459471178002177 Test2 Acc 0.6194423983805375 Test 3 Acc 0.5948112476327105 AVG Test ACC 0.6200669212711553\n",
      "==================================================\n",
      "Epoch 29 Train Acc: 0.7229939496902368, Test1 Acc: 0.6251355212536741 Test2 Acc 0.6129441607765076 Test 3 Acc 0.5805071089008759 AVG Test ACC 0.6061955969770191\n",
      "==================================================\n",
      "Epoch 30 Train Acc: 0.7236496809956984, Test1 Acc: 0.6337417854840091 Test2 Acc 0.623132960588219 Test 3 Acc 0.5787268633528811 AVG Test ACC 0.6118672031417031\n",
      "==================================================\n",
      "Epoch 31 Train Acc: 0.729627498953354, Test1 Acc: 0.6372779561147716 Test2 Acc 0.6243965304982758 Test 3 Acc 0.5936756318756853 AVG Test ACC 0.6184500394962442\n",
      "==================================================\n",
      "Epoch 32 Train Acc: 0.7353450525258047, Test1 Acc: 0.6422426778516689 Test2 Acc 0.6313539183974578 Test 3 Acc 0.5775079516704898 AVG Test ACC 0.6170348493065388\n",
      "==================================================\n",
      "Epoch 33 Train Acc: 0.7410391809683917, Test1 Acc: 0.6329663477921313 Test2 Acc 0.618230529314475 Test 3 Acc 0.5811210593186722 AVG Test ACC 0.6107726454750928\n",
      "==================================================\n",
      "Epoch 34 Train Acc: 0.7466155003661195, Test1 Acc: 0.6381992976002797 Test2 Acc 0.6093931029369194 Test 3 Acc 0.5928336231777994 AVG Test ACC 0.6134753412383328\n",
      "==================================================\n",
      "Epoch 35 Train Acc: 0.7511301004366819, Test1 Acc: 0.6437142619332233 Test2 Acc 0.6192724442796882 Test 3 Acc 0.5832404861074546 AVG Test ACC 0.6154090641067888\n",
      "==================================================\n",
      "Epoch 36 Train Acc: 0.7499104431035265, Test1 Acc: 0.640092533050285 Test2 Acc 0.6111773986561414 Test 3 Acc 0.5852052393456605 AVG Test ACC 0.6121583903506956\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 4.00 GiB total capacity; 669.61 MiB already allocated; 1.97 GiB free; 728.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6272/532501625.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mtrain_acces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mtest_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_lengths1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mtest_acc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_y1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6272/3782444875.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inp, lens)\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mpad_embed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mpad_embed_pack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpad_embed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mht\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mct\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpad_embed_pack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mht\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mht\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_layer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    692\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    693\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 694\u001b[1;33m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0m\u001b[0;32m    695\u001b[0m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0;32m    696\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 4.00 GiB total capacity; 669.61 MiB already allocated; 1.97 GiB free; 728.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "train_acces,train_losses = [],[]\n",
    "MAX_AVG_TEST_ACC = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_outputs,train_labels = [],[]\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "    for inputs,labels,lengths in loader_train:\n",
    "        sample_weight = torch.from_numpy(compute_sample_weight(class_weight='balanced',y=labels)).to(device)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)   \n",
    "        model.zero_grad()\n",
    "        output = model(inputs,lengths)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss = (loss * sample_weight).mean()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_outputs.append(output.cpu().detach().numpy())\n",
    "        train_labels.append(labels.cpu().detach().numpy())\n",
    "        del inputs, labels, output, loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        train_outputs = np.vstack(train_outputs)\n",
    "        train_y = np.vstack(train_labels)\n",
    "        train_loss = criterion(torch.from_numpy(train_outputs.squeeze()),torch.from_numpy(train_y.astype(float)) ).mean()\n",
    "        train_losses.append(train_loss)\n",
    "        train_acc = f1_score( np.argmax(train_y,axis=-1) , np.argmax(train_outputs,axis=-1) ,average='macro')\n",
    "        train_acces.append(train_acc)\n",
    "\n",
    "        test_outputs = model(test_x1.to(device),test_lengths1).cpu()\n",
    "        test_acc1 = f1_score( np.argmax(test_y1,axis=-1) , torch.argmax(test_outputs,dim=-1) ,average='macro')\n",
    "        \n",
    "        test_outputs = model(test_x2.to(device),test_lengths2).cpu()\n",
    "        test_acc2 = f1_score( np.argmax(test_y2,axis=-1) , torch.argmax(test_outputs,dim=-1) ,average='macro')\n",
    "        \n",
    "        test_outputs = model(test_x3.to(device),test_lengths3).cpu()\n",
    "        test_acc3 = f1_score( np.argmax(test_y3,axis=-1) , torch.argmax(test_outputs,dim=-1) ,average='macro')\n",
    "        \n",
    "        avg_test_acc = np.mean([test_acc1,test_acc2,test_acc3])\n",
    "        if avg_test_acc > MAX_AVG_TEST_ACC:\n",
    "            print(\"AVERAGE MAXIMUM TEST ACC IS\",avg_test_acc)\n",
    "            torch.save(model.state_dict(),'Pytorch_lstm_model.pt' )\n",
    "            MAX_AVG_TEST_ACC = avg_test_acc\n",
    "\n",
    "        print('''Epoch {epoch} Train Acc: {train_acc}, Test1 Acc: {test1_acc} Test2 Acc {test2_acc} Test 3 Acc {test3_acc} AVG Test ACC {avg_acc}'''\\\n",
    "              .format(epoch=epoch,train_acc=train_acc,test1_acc=test_acc1,test2_acc=test_acc2,test3_acc=test_acc3,avg_acc=avg_test_acc))\n",
    "        del test_outputs,train_outputs, train_labels,train_loss\n",
    "        torch.cuda.empty_cache()\n",
    "        print(25*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c665d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aabe8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
