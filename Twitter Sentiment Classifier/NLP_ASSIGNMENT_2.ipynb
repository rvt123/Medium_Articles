{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "from time import sleep\n",
    "import os\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('D:/MSC/NLP/NLP_ASS2/semeval-tweets/')\n",
    "import twokenize\n",
    "BASE_DIR = 'D:/MSC/NLP/NLP_ASS2/semeval-tweets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Contractions\n",
    "file = open(BASE_DIR + 'CONTRACTIONS.pkl', \"rb\")\n",
    "CONTRACTION_MAP = pickle.load(file)\n",
    "sleep(1)\n",
    "del file\n",
    "contraction = sorted(CONTRACTION_MAP, key=len, reverse=True)\n",
    "c_re = re.compile('(%s)' % '|'.join(contraction))\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return CONTRACTION_MAP[match.group(0)]\n",
    "    return c_re.sub(replace, text.lower())\n",
    "\n",
    "## Emojis Regex\n",
    "file = open(BASE_DIR + 'EMOJIS.pkl', \"rb\")\n",
    "EMOJIS = pickle.load(file)\n",
    "sleep(1)\n",
    "del file\n",
    "emojis = sorted(EMOJIS, key=len, reverse=True)\n",
    "pattern = '|'.join(re.escape(u) for u in emojis)\n",
    "regex = re.compile(pattern, re.U)\n",
    "\n",
    "## Regex expression for Booster Increase and Decrease words\n",
    "B_INC = ['exceptionally', 'substantially', 'considerable', 'considerably', 'particularly', 'tremendously', 'unbelievably',\\\n",
    "         'exceptional', 'absolutely', 'completely', 'enormously', 'especially', 'fabulously', 'incredible', 'incredibly',\\\n",
    "         'remarkably', 'thoroughly', 'tremendous', 'amazingly', 'decidedly', 'extremely', 'intensely', 'unusually',\\\n",
    "         'enormous', 'entirely', 'flipping', 'fracking', 'fricking', 'frigging', 'awfully', 'extreme', 'flippin',\\\n",
    "         'frackin', 'frickin', 'friggin', 'fucking', 'fugging', 'greatly', 'majorly', 'totally', 'utterly', 'deeply',\\\n",
    "         'effing', 'fuckin', 'fuggin', 'highly', 'hugely', 'purely', 'really', 'fully', 'hella', 'major', 'quite',\\\n",
    "         'total', 'utter', 'more', 'most', 'uber', 'very', 'so']\n",
    "B_DEC = ['almost', 'barely', 'hardly', 'just enough', 'kind of', 'kinda', 'kindof', 'kind-of', 'less', 'little',\\\n",
    "         'marginal', 'marginally', 'occasional', 'occasionally', 'partly', 'scarce', 'scarcely', 'slight', 'slightly',\\\n",
    "         'somewhat', 'sort of', 'sorta', 'sortof', 'sort-of']\n",
    "pattern_binc = '|'.join(re.escape(u) for u in B_INC)\n",
    "pattern_bdec = '|'.join(re.escape(u) for u in B_DEC)\n",
    "\n",
    "## Reading EXICON DICT \n",
    "file = open(BASE_DIR + 'VADER_LEXICONS_SCORE.pkl', \"rb\")\n",
    "LEXICON_DICT = pickle.load(file)\n",
    "sleep(1)\n",
    "del file\n",
    "\n",
    "\n",
    "file = open(BASE_DIR + 'POSITIVE.pkl', \"rb\")\n",
    "POS_LIST = pickle.load(file)\n",
    "sleep(1)\n",
    "del file\n",
    "POS_LIST = sorted(POS_LIST, key=len, reverse=True)\n",
    "pattern_pos = '|'.join(re.escape(u) for u in POS_LIST)\n",
    "\n",
    "file = open(BASE_DIR + 'NEGATIVE.pkl', \"rb\")\n",
    "NEG_LIST = pickle.load(file)\n",
    "sleep(1)\n",
    "del file\n",
    "NEG_LIST = sorted(NEG_LIST, key=len, reverse=True)\n",
    "pattern_neg = '|'.join(re.escape(u) for u in NEG_LIST)\n",
    "\n",
    "file = open(BASE_DIR + 'BAD.pkl', \"rb\")\n",
    "BAD_LIST = pickle.load(file)\n",
    "sleep(1)\n",
    "del file\n",
    "BAD_LIST = sorted(BAD_LIST, key=len, reverse=True)\n",
    "pattern_bad = '|'.join(re.escape(u) for u in BAD_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_tok_val(sentence):\n",
    "    pos_tok = []\n",
    "    neg_tok = []\n",
    "    tokens = twokenize.tokenizeRawTweetText(sentence)\n",
    "    ln_all_tokens = len(tokens)\n",
    "    for toke in tokens:\n",
    "        val = LEXICON_DICT.get(toke)\n",
    "        if val:\n",
    "            if val >0:\n",
    "                pos_tok.append(val)\n",
    "            else:\n",
    "                neg_tok.append(val)\n",
    "    \n",
    "    return tokens,ln_all_tokens,len(pos_tok),len(neg_tok),sum(pos_tok),sum(neg_tok)\n",
    "\n",
    "def clean_data(data,col,re_inc_boostr,re_dec_boostr,re_pos,re_neg,re_bad):\n",
    "    dataframe = data.copy()\n",
    "    link_regex = re.compile(r'(?:ftp|https?|www|file)\\.?:?[//|\\\\\\\\]?[\\w\\d:#@%/;$()~_?\\+-=\\\\\\&]+\\.[\\w\\d:#@%/;$~_?\\+-=\\\\\\&]+')\n",
    "    dataframe = dataframe.assign(**dict(zip([col,'num_link'], zip(*dataframe['text'].apply(lambda x: re.subn(link_regex,'LINK',x) ) ))))\n",
    "    dataframe = dataframe.assign(**dict(zip([col,'num_usermention'], zip(*dataframe[col].apply(lambda x: re.subn(r'@[\\w]*','USERMENTION',x)) ))))\n",
    "    dataframe = dataframe.assign(**dict(zip([col,'num_hashtag'], zip(*dataframe[col].apply(lambda x: re.subn(r'#[\\w]*','HASHTAG',x) ) ))))\n",
    "    dataframe = dataframe.assign(**dict(zip([col,'num_emoji'], zip(*dataframe[col].apply(lambda x: re.subn(regex, lambda m: EMOJIS.get(m.group(), 'EMOJI') , x) )))))\n",
    "    dataframe[col] = dataframe[col].apply(lambda x : re.sub(r'(.)\\1{2,}', r'\\1',x)) # make looong as long\n",
    "    dataframe[col] = dataframe[col].apply(lambda x : expandContractions(x) ) # expand contracts\n",
    "    dataframe[col] = dataframe[col].str.lower()\n",
    "    dataframe['NUM_INC_BOOSTR'] = dataframe[col].apply(lambda x : len(re.findall(re_inc_boostr,x)) ) # Booster Increasing Words\n",
    "    dataframe['NUM_DEC_BOOSTR'] = dataframe[col].apply(lambda x : len(re.findall(re_dec_boostr,x)) ) # Booster Increasing Words\n",
    "    dataframe['NUM_POS_WORDS'] = dataframe[col].apply(lambda x : len(re.findall(re_pos,x)) ) # Number of Positive words\n",
    "    dataframe['NUM_NEG_WORDS'] = dataframe[col].apply(lambda x : len(re.findall(re_neg,x)) ) # Number of Positive words\n",
    "    dataframe['NUM_BAD_WORDS'] = dataframe[col].apply(lambda x : len(re.findall(re_bad,x)) ) # Number of Positive words\n",
    "    dataframe = dataframe.assign(**dict(zip([col,'num_exclaim'], zip(*dataframe[col].apply(lambda x: re.subn(r\"[!]\",'',x)  ) ))))  \n",
    "    dataframe = dataframe.assign(**dict(zip([col,'num_punct'], zip(*dataframe[col].apply(lambda x: re.subn(r\"['\\\"“”‘’.?!…,:;]\",'',x)  ) ))))  \n",
    "    dataframe = dataframe.assign(**dict(zip([col,'num_extra'], zip(*dataframe[col].apply(lambda x: re.subn(r'[^a-zA-Z\\s0-9]','',x)  ) ))))  \n",
    "    dataframe = dataframe.assign(**dict(zip([col,'nm_all_tok','nm_pos_tok','nm_neg_tok','sm_pos_tok','sum_nrg_tok'], zip(*dataframe[col].apply(lambda x: return_tok_val(x) ) ))))\n",
    "    dataframe['lex_tok'] = dataframe['nm_pos_tok'] + dataframe['nm_neg_tok']\n",
    "    dataframe['total_lex_score'] = dataframe['sm_pos_tok'] + dataframe['sum_nrg_tok']\n",
    "    dataframe['r_sco/tok'] = dataframe['total_lex_score']/dataframe['lex_tok']\n",
    "    for col in dataframe.columns.difference(['clean_text','text','sentiment']):\n",
    "        dataframe[col] += abs(dataframe[col].min())\n",
    "    \n",
    "    dataframe = dataframe.fillna(0)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(BASE_DIR + 'twitter-training-data.txt',sep='\\t',names=['ID','sentiment','text']).drop('ID',axis=1)\n",
    "df_dev = pd.read_csv(BASE_DIR + 'twitter-dev-data.txt',sep='\\t',names=['ID','sentiment','text']).drop('ID',axis=1)\n",
    "# df_train = pd.concat([df_train,df_dev]).reset_index(drop=True)\n",
    "# del df_dev\n",
    "df_test = pd.read_csv(BASE_DIR + 'twitter-test1.txt',sep='\\t',names=['ID','sentiment','text']).drop('ID',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     0.460823\n",
       "positive    0.354462\n",
       "negative    0.184715\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['sentiment'].value_counts()/len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     0.4595\n",
       "positive    0.3515\n",
       "negative    0.1890\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['sentiment'].value_counts()/len(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     0.425942\n",
       "positive    0.416313\n",
       "negative    0.157746\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['sentiment'].value_counts()/len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raghu\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "C:\\Users\\raghu\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "df_train['clean_text'] = df_train['text']\n",
    "df_test['clean_text'] = df_test['text']\n",
    "\n",
    "df_train = clean_data(df_train,'clean_text',pattern_binc,pattern_bdec,pattern_pos,pattern_neg,pattern_bad)\n",
    "df_test = clean_data(df_test,'clean_text',pattern_binc,pattern_bdec,pattern_pos,pattern_neg,pattern_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace = {'positive': 0, 'negative': 1, 'neutral': 2}\n",
    "df_train['sentiment'] = df_train['sentiment'].replace(replace)\n",
    "df_test['sentiment'] = df_test['sentiment'].replace(replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=lambda x: x,preprocessor=lambda x: x,ngram_range=(1,3),min_df=10)\n",
    "tfidf_vectorizer_vectors_train = tfidf.fit_transform(df_train.loc[:,'clean_text'].values)\n",
    "tfidf_vectorizer_vectors_test = tfidf.transform(df_test.loc[:,'clean_text'].values)\n",
    "pickle.dump(tfidf.vocabulary_, open(BASE_DIR + 'TFIDF.pkl', \"wb\"))\n",
    "sleep(1)\n",
    "\n",
    "train_x = tfidf_vectorizer_vectors_train.toarray()\n",
    "test_x = tfidf_vectorizer_vectors_test.toarray()\n",
    "train_y = df_train['sentiment'].values\n",
    "test_y = df_test['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.concatenate((train_x,df_train[df_train.columns.difference(['sentiment', 'text', 'clean_text'])].values),axis=1)\n",
    "test_x = np.concatenate((test_x,df_test[df_test.columns.difference(['sentiment', 'text', 'clean_text'])].values),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6218149693013134\n",
      "0.6661988047388181\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB().fit(train_x,train_y)  ## 6396  and ## 6912\n",
    "predicted = model.predict(train_x)\n",
    "print(f1_score(train_y,predicted,average='macro'))\n",
    "predicted_test = model.predict(test_x)\n",
    "print(f1_score(test_y,predicted_test,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.697935994449718\n",
      "0.6173130722383504\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB().fit(train_x,train_y)  ## 6396  and ## 6912\n",
    "predicted = model.predict(train_x)\n",
    "print(f1_score(train_y,predicted,average='weighted'))\n",
    "predicted_test = model.predict(test_x)\n",
    "print(f1_score(test_y,predicted_test,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raghu\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.664229406489832\n"
     ]
    }
   ],
   "source": [
    "## Without is 66 and with is 65 something\n",
    "\n",
    "df_test1 = pd.read_csv(BASE_DIR + 'twitter-test2.txt',sep='\\t',names=['ID','sentiment','text']).drop('ID',axis=1)\n",
    "df_test1['clean_text'] = df_test1['text']\n",
    "df_test1 = clean_data(df_test1,'clean_text',pattern_binc,pattern_bdec,pattern_pos,pattern_neg,pattern_bad)\n",
    "# df_test1 = df_test1.drop('num_exclaim',axis=1)\n",
    "df_test1['sentiment'] = df_test1['sentiment'].replace(replace)\n",
    "tfidf_vectorizer_vectors_test1 = tfidf.transform(df_test1.loc[:,'clean_text'].values)\n",
    "test_x1 = tfidf_vectorizer_vectors_test1.toarray()\n",
    "test_y1 = df_test1['sentiment'].values\n",
    "test_x1 = np.concatenate((test_x1,df_test1[df_test1.columns.difference(['sentiment', 'text', 'clean_text'])].values),axis=1)\n",
    "predicted_test1 = model.predict(test_x1)\n",
    "print(f1_score(test_y1,predicted_test1,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raghu\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6326952173891384\n"
     ]
    }
   ],
   "source": [
    "## Without  6304 , With 58 something\n",
    "df_test1 = pd.read_csv(BASE_DIR + 'twitter-test3.txt',sep='\\t',names=['ID','sentiment','text']).drop('ID',axis=1)\n",
    "df_test1['clean_text'] = df_test1['text']\n",
    "df_test1 = clean_data(df_test1,'clean_text',pattern_binc,pattern_bdec,pattern_pos,pattern_neg,pattern_bad)\n",
    "# df_test1 = df_test1.drop('num_exclaim',axis=1)\n",
    "df_test1['sentiment'] = df_test1['sentiment'].replace(replace)\n",
    "tfidf_vectorizer_vectors_test1 = tfidf.transform(df_test1.loc[:,'clean_text'].values)\n",
    "test_x1 = tfidf_vectorizer_vectors_test1.toarray()\n",
    "test_y1 = df_test1['sentiment'].values\n",
    "test_x1 = np.concatenate((test_x1,df_test1[df_test1.columns.difference(['sentiment', 'text', 'clean_text'])].values),axis=1)\n",
    "predicted_test1 = model.predict(test_x1)\n",
    "print(f1_score(test_y1,predicted_test1,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.to_pickle(BASE_DIR + 'train_data.pkl')\n",
    "# df_test.to_pickle(BASE_DIR + 'test_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_dict = {}\n",
    "# with open(BASE_DIR + \"glove.6B.100d.txt\", 'r', encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         values = line.split()\n",
    "#         token = values[0]\n",
    "#         vector = np.asarray(values[1:], \"float32\")\n",
    "#         embeddings_dict[token] = vector\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# count_vector = CountVectorizer(tokenizer=lambda x: x,preprocessor=lambda x: x)\n",
    "# count_vector.fit(data_train['clean_text'])\n",
    "\n",
    "# vocab = count_vector.vocabulary_\n",
    "\n",
    "# min_list,max_list = [],[]\n",
    "# while len(vocab) > 0:\n",
    "#     min_word = min(vocab,key=vocab.get)\n",
    "#     min_list.append(min_word)\n",
    "#     vocab.pop(min_word)\n",
    "#     if len(vocab) == 0:\n",
    "#         break\n",
    "#     max_word = max(vocab,key=vocab.get)\n",
    "#     max_list.append(max_word)\n",
    "#     vocab.pop(max_word)\n",
    "    \n",
    "# new_vocab = {}\n",
    "# while len(new_vocab) != 4998:\n",
    "#     word = max_list[-1]\n",
    "#     vector = embeddings_dict.get(word)\n",
    "#     if vector is None :\n",
    "#         max_list.pop()\n",
    "#     else:\n",
    "#         new_vocab[word] = vector\n",
    "#         max_list.pop()\n",
    "#         if len(new_vocab) == 4998:\n",
    "#             break\n",
    "#     word = min_list[-1]\n",
    "#     vector = embeddings_dict.get(word)\n",
    "#     if vector is None:\n",
    "#         min_list.pop()\n",
    "#     else:\n",
    "#         new_vocab[word] = vector\n",
    "#         min_list.pop()\n",
    "#         if len(new_vocab) == 4998:\n",
    "#             break\n",
    "            \n",
    "# pickle.dump(new_vocab,open(BASE_DIR + 'LSTM_VOCAB.pkl', \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
